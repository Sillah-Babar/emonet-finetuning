{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:55:00.496736Z",
     "iopub.status.busy": "2025-10-23T18:55:00.496447Z",
     "iopub.status.idle": "2025-10-23T18:55:00.682890Z",
     "shell.execute_reply": "2025-10-23T18:55:00.682118Z",
     "shell.execute_reply.started": "2025-10-23T18:55:00.496714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_emonet_emotic_full.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, mean_absolute_error,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================\n",
    "# MODEL DEFINITION (from previous response)\n",
    "# ============================================\n",
    "def conv3x3(in_planes, out_planes, strd=1, padding=1, bias=False):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3,\n",
    "                     stride=strd, padding=padding, bias=bias)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = conv3x3(in_planes, int(out_planes / 2))\n",
    "        self.bn2 = nn.BatchNorm2d(int(out_planes / 2))\n",
    "        self.conv2 = conv3x3(int(out_planes / 2), int(out_planes / 4))\n",
    "        self.bn3 = nn.BatchNorm2d(int(out_planes / 4))\n",
    "        self.conv3 = conv3x3(int(out_planes / 4), int(out_planes / 4))\n",
    "\n",
    "        if in_planes != out_planes:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.BatchNorm2d(in_planes),\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, bias=False),\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out1 = self.bn1(x)\n",
    "        out1 = F.relu(out1, True)\n",
    "        out1 = self.conv1(out1)\n",
    "\n",
    "        out2 = self.bn2(out1)\n",
    "        out2 = F.relu(out2, True)\n",
    "        out2 = self.conv2(out2)\n",
    "\n",
    "        out3 = self.bn3(out2)\n",
    "        out3 = F.relu(out3, True)\n",
    "        out3 = self.conv3(out3)\n",
    "\n",
    "        out3 = torch.cat((out1, out2, out3), 1)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(residual)\n",
    "\n",
    "        out3 += residual\n",
    "        return out3\n",
    "\n",
    "class HourGlass(nn.Module):\n",
    "    def __init__(self, num_modules, depth, num_features):\n",
    "        super(HourGlass, self).__init__()\n",
    "        self.num_modules = num_modules\n",
    "        self.depth = depth\n",
    "        self.features = num_features\n",
    "        self._generate_network(self.depth)\n",
    "\n",
    "    def _generate_network(self, level):\n",
    "        self.add_module('b1_' + str(level), ConvBlock(256, 256))\n",
    "        self.add_module('b2_' + str(level), ConvBlock(256, 256))\n",
    "\n",
    "        if level > 1:\n",
    "            self._generate_network(level - 1)\n",
    "        else:\n",
    "            self.add_module('b2_plus_' + str(level), ConvBlock(256, 256))\n",
    "\n",
    "        self.add_module('b3_' + str(level), ConvBlock(256, 256))\n",
    "\n",
    "    def _forward(self, level, inp):\n",
    "        up1 = inp\n",
    "        up1 = self._modules['b1_' + str(level)](up1)\n",
    "\n",
    "        low1 = F.max_pool2d(inp, 2, stride=2)\n",
    "        low1 = self._modules['b2_' + str(level)](low1)\n",
    "\n",
    "        if level > 1:\n",
    "            low2 = self._forward(level - 1, low1)\n",
    "        else:\n",
    "            low2 = low1\n",
    "            low2 = self._modules['b2_plus_' + str(level)](low2)\n",
    "\n",
    "        low3 = low2\n",
    "        low3 = self._modules['b3_' + str(level)](low3)\n",
    "        up2 = F.interpolate(low3, scale_factor=2, mode='nearest')\n",
    "        return up1 + up2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward(self.depth, x)\n",
    "\n",
    "class EmoNetSingleLabel26(nn.Module):\n",
    "    def __init__(self, num_modules=2, n_expression=26, n_reg=2, n_blocks=4, attention=True):\n",
    "        super(EmoNetSingleLabel26, self).__init__()\n",
    "        self.num_modules = num_modules\n",
    "        self.n_expression = n_expression\n",
    "        self.n_reg = n_reg\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = ConvBlock(64, 128)\n",
    "        self.conv3 = ConvBlock(128, 128)\n",
    "        self.conv4 = ConvBlock(128, 256)\n",
    "\n",
    "        for hg_module in range(self.num_modules):\n",
    "            self.add_module('m' + str(hg_module), HourGlass(1, 4, 256))\n",
    "            self.add_module('top_m_' + str(hg_module), ConvBlock(256, 256))\n",
    "            self.add_module('conv_last' + str(hg_module),\n",
    "                            nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\n",
    "            self.add_module('bn_end' + str(hg_module), nn.BatchNorm2d(256))\n",
    "            self.add_module('l' + str(hg_module), \n",
    "                            nn.Conv2d(256, 68, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "            if hg_module < self.num_modules - 1:\n",
    "                self.add_module('bl' + str(hg_module), \n",
    "                                nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\n",
    "                self.add_module('al' + str(hg_module), \n",
    "                                nn.Conv2d(68, 256, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "        if self.attention:\n",
    "            n_in_features = 256 * (num_modules + 1)\n",
    "        else:\n",
    "            n_in_features = 256 * (num_modules + 1) + 68\n",
    "        \n",
    "        n_features = [(256, 256)] * n_blocks\n",
    "\n",
    "        self.emo_convs = []\n",
    "        self.conv1x1_input_emo_2 = nn.Conv2d(n_in_features, 256, kernel_size=1, stride=1, padding=0)\n",
    "        for in_f, out_f in n_features:\n",
    "            self.emo_convs.append(ConvBlock(in_f, out_f))\n",
    "            self.emo_convs.append(nn.MaxPool2d(2, 2))\n",
    "        self.emo_net_2 = nn.Sequential(*self.emo_convs)\n",
    "        self.avg_pool_2 = nn.AvgPool2d(4)\n",
    "        \n",
    "        self.emo_fc_shared = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.emotion_head = nn.Linear(128, self.n_expression)\n",
    "        self.valence_head = nn.Linear(128, 1)\n",
    "        self.arousal_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)), True)\n",
    "        x = F.max_pool2d(self.conv2(x), 2, stride=2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        previous = x\n",
    "        hg_features = []\n",
    "\n",
    "        for i in range(self.num_modules):\n",
    "            hg = self._modules['m' + str(i)](previous)\n",
    "            ll = hg\n",
    "            ll = self._modules['top_m_' + str(i)](ll)\n",
    "            ll = F.relu(self._modules['bn_end' + str(i)]\n",
    "                        (self._modules['conv_last' + str(i)](ll)), True)\n",
    "            tmp_out = self._modules['l' + str(i)](ll)\n",
    "\n",
    "            if i < self.num_modules - 1:\n",
    "                ll = self._modules['bl' + str(i)](ll)\n",
    "                tmp_out_ = self._modules['al' + str(i)](tmp_out)\n",
    "                previous = previous + ll + tmp_out_\n",
    "\n",
    "            hg_features.append(ll)\n",
    "\n",
    "        hg_features_cat = torch.cat(tuple(hg_features), dim=1)\n",
    "\n",
    "        if self.attention:\n",
    "            mask = torch.sum(tmp_out, dim=1, keepdim=True)\n",
    "            mask = torch.sigmoid(mask)\n",
    "            hg_features_cat = hg_features_cat * mask\n",
    "            emo_feat = torch.cat((x, hg_features_cat), dim=1)\n",
    "        else:\n",
    "            emo_feat = torch.cat([x, hg_features_cat, tmp_out], dim=1)\n",
    "\n",
    "        emo_feat_conv1D = self.conv1x1_input_emo_2(emo_feat)\n",
    "        final_features = self.emo_net_2(emo_feat_conv1D)\n",
    "        final_features = self.avg_pool_2(final_features)\n",
    "        batch_size = final_features.shape[0]\n",
    "        final_features = final_features.view(batch_size, -1)\n",
    "        \n",
    "        shared_features = self.emo_fc_shared(final_features)\n",
    "        \n",
    "        emotion_logits = self.emotion_head(shared_features)\n",
    "        valence = torch.tanh(self.valence_head(shared_features)).squeeze(1)\n",
    "        arousal = torch.tanh(self.arousal_head(shared_features)).squeeze(1)\n",
    "\n",
    "        return {\n",
    "            'heatmap': tmp_out,\n",
    "            'expression': emotion_logits,\n",
    "            'valence': valence,\n",
    "            'arousal': arousal\n",
    "        }\n",
    "\n",
    "    def load_pretrained_emonet(self, pretrained_path, freeze_backbone=True):\n",
    "        print(f\"Loading pretrained EmoNet from {pretrained_path}\")\n",
    "        \n",
    "        try:\n",
    "            pretrained_state = torch.load(pretrained_path, map_location='cpu')\n",
    "            \n",
    "            if 'state_dict' in pretrained_state:\n",
    "                pretrained_state = pretrained_state['state_dict']\n",
    "            \n",
    "            pretrained_state = {k.replace('module.', ''): v \n",
    "                               for k, v in pretrained_state.items()}\n",
    "            \n",
    "            model_state = self.state_dict()\n",
    "            \n",
    "            compatible_weights = {}\n",
    "            incompatible_keys = []\n",
    "            \n",
    "            for k, v in pretrained_state.items():\n",
    "                if 'emo_fc_2' in k:\n",
    "                    incompatible_keys.append(k)\n",
    "                    continue\n",
    "                \n",
    "                if k in model_state and model_state[k].shape == v.shape:\n",
    "                    compatible_weights[k] = v\n",
    "                else:\n",
    "                    incompatible_keys.append(k)\n",
    "            \n",
    "            model_state.update(compatible_weights)\n",
    "            self.load_state_dict(model_state)\n",
    "            \n",
    "            print(f\"‚úì Loaded {len(compatible_weights)} layers from pretrained model\")\n",
    "            print(f\"‚úó Skipped {len(incompatible_keys)} incompatible layers\")\n",
    "            \n",
    "            if freeze_backbone:\n",
    "                self.freeze_backbone()\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Failed to load pretrained weights: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        frozen_params = 0\n",
    "        trainable_params = 0\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            if any(head in name for head in ['emotion_head', 'valence_head', \n",
    "                                              'arousal_head', 'emo_fc_shared']):\n",
    "                param.requires_grad = True\n",
    "                trainable_params += param.numel()\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                frozen_params += param.numel()\n",
    "        \n",
    "        print(f\"‚úì Frozen backbone:\")\n",
    "        print(f\"    Frozen: {frozen_params:,} | Trainable: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:55:14.612200Z",
     "iopub.status.busy": "2025-10-23T18:55:14.611925Z",
     "iopub.status.idle": "2025-10-23T18:55:14.701931Z",
     "shell.execute_reply": "2025-10-23T18:55:14.701164Z",
     "shell.execute_reply.started": "2025-10-23T18:55:14.612181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_emonet_emotic_fixed.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    f1_score, accuracy_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, mean_absolute_error,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "EMOTION_CLASSES = {\n",
    "    0: \"Peace\", 1: \"Affection\", 2: \"Esteem\", 3: \"Anticipation\", 4: \"Engagement\",\n",
    "    5: \"Confidence\", 6: \"Happiness\", 7: \"Pleasure\", 8: \"Excitement\", 9: \"Surprise\",\n",
    "    10: \"Sympathy\", 11: \"Doubt/Confusion\", 12: \"Disconnection\", 13: \"Fatigue\",\n",
    "    14: \"Embarrassment\", 15: \"Yearning\", 16: \"Disapproval\", 17: \"Aversion\",\n",
    "    18: \"Annoyance\", 19: \"Anger\", 20: \"Sensitivity\", 21: \"Sadness\",\n",
    "    22: \"Disquietment\", 23: \"Fear\", 24: \"Pain\", 25: \"Suffering\"\n",
    "}\n",
    "EMOTION_COLUMNS = [\n",
    "    'Peace', 'Affection', 'Esteem', 'Anticipation', 'Engagement',\n",
    "    'Confidence', 'Happiness', 'Pleasure', 'Excitement', 'Surprise',\n",
    "    'Sympathy', 'Doubt/Confusion', 'Disconnection', 'Fatigue',\n",
    "    'Embarrassment', 'Yearning', 'Disapproval', 'Aversion',\n",
    "    'Annoyance', 'Anger', 'Sensitivity', 'Sadness', 'Disquietment',\n",
    "    'Fear', 'Pain', 'Suffering'\n",
    "]\n",
    "# [Keep all your existing model code: EmoNetSingleLabel26, ConvBlock, HourGlass, etc.]\n",
    "# [Keep EMOTION_CLASSES and EMOTION_COLUMNS definitions]\n",
    "# [Keep MetricsTracker class]\n",
    "# [Keep plotting functions]\n",
    "# ======================\n",
    "class MetricsTracker:\n",
    "    def __init__(self, num_classes=26, class_names=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.class_names = class_names or [str(i) for i in range(num_classes)]\n",
    "        \n",
    "        self.all_preds = []\n",
    "        self.all_labels = []\n",
    "        self.all_valence_preds = []\n",
    "        self.all_valence_labels = []\n",
    "        self.all_arousal_preds = []\n",
    "        self.all_arousal_labels = []\n",
    "    \n",
    "    def update(self, preds, labels, valence_preds=None, valence_labels=None, \n",
    "               arousal_preds=None, arousal_labels=None):\n",
    "        self.all_preds.extend(preds.cpu().numpy())\n",
    "        self.all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        if valence_preds is not None:\n",
    "            self.all_valence_preds.extend(valence_preds.cpu().numpy())\n",
    "            self.all_valence_labels.extend(valence_labels.cpu().numpy())\n",
    "        \n",
    "        if arousal_preds is not None:\n",
    "            self.all_arousal_preds.extend(arousal_preds.cpu().numpy())\n",
    "            self.all_arousal_labels.extend(arousal_labels.cpu().numpy())\n",
    "    \n",
    "    def compute(self):\n",
    "        preds = np.array(self.all_preds)\n",
    "        labels = np.array(self.all_labels)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(labels, preds),\n",
    "            'f1_macro': f1_score(labels, preds, average='macro', zero_division=0),\n",
    "            'f1_weighted': f1_score(labels, preds, average='weighted', zero_division=0),\n",
    "            'precision_macro': precision_score(labels, preds, average='macro', zero_division=0),\n",
    "            'recall_macro': recall_score(labels, preds, average='macro', zero_division=0)\n",
    "        }\n",
    "        \n",
    "        if self.all_valence_preds:\n",
    "            valence_preds = np.array(self.all_valence_preds)\n",
    "            valence_labels = np.array(self.all_valence_labels)\n",
    "            metrics['valence_mae'] = mean_absolute_error(valence_labels, valence_preds)\n",
    "            metrics['valence_rmse'] = np.sqrt(mean_squared_error(valence_labels, valence_preds))\n",
    "            metrics['valence_r2'] = r2_score(valence_labels, valence_preds)\n",
    "        \n",
    "        if self.all_arousal_preds:\n",
    "            arousal_preds = np.array(self.all_arousal_preds)\n",
    "            arousal_labels = np.array(self.all_arousal_labels)\n",
    "            metrics['arousal_mae'] = mean_absolute_error(arousal_labels, arousal_preds)\n",
    "            metrics['arousal_rmse'] = np.sqrt(mean_squared_error(arousal_labels, arousal_preds))\n",
    "            metrics['arousal_r2'] = r2_score(arousal_labels, arousal_preds)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def print_metrics(self, metrics_dict, phase=\"\"):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"{phase} Metrics\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Accuracy:       {metrics_dict['accuracy']*100:.2f}%\")\n",
    "        print(f\"F1 (Macro):     {metrics_dict['f1_macro']:.4f}\")\n",
    "        print(f\"F1 (Weighted):  {metrics_dict['f1_weighted']:.4f}\")\n",
    "        print(f\"Precision:      {metrics_dict['precision_macro']:.4f}\")\n",
    "        print(f\"Recall:         {metrics_dict['recall_macro']:.4f}\")\n",
    "        \n",
    "        if 'valence_mae' in metrics_dict:\n",
    "            print(f\"\\nValence:\")\n",
    "            print(f\"  MAE:  {metrics_dict['valence_mae']:.4f}\")\n",
    "            print(f\"  RMSE: {metrics_dict['valence_rmse']:.4f}\")\n",
    "            print(f\"  R¬≤:   {metrics_dict['valence_r2']:.4f}\")\n",
    "        \n",
    "        if 'arousal_mae' in metrics_dict:\n",
    "            print(f\"\\nArousal:\")\n",
    "            print(f\"  MAE:  {metrics_dict['arousal_mae']:.4f}\")\n",
    "            print(f\"  RMSE: {metrics_dict['arousal_rmse']:.4f}\")\n",
    "            print(f\"  R¬≤:   {metrics_dict['arousal_r2']:.4f}\")\n",
    "        \n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# PLOTTING FUNCTIONS\n",
    "# ============================================\n",
    "def plot_training_progress(history, save_path):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Training Progress', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label='Train', linewidth=2)\n",
    "    axes[0, 0].plot(history['val_loss'], label='Val', linewidth=2)\n",
    "    axes[0, 0].set_title('Total Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(history['train_acc'], label='Train', linewidth=2)\n",
    "    axes[0, 1].plot(history['val_acc'], label='Val', linewidth=2)\n",
    "    axes[0, 1].set_title('Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Score\n",
    "    axes[0, 2].plot(history['train_f1'], label='Train', linewidth=2)\n",
    "    axes[0, 2].plot(history['val_f1'], label='Val', linewidth=2)\n",
    "    axes[0, 2].set_title('F1 Score (Macro)')\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('F1 Score')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Component Losses\n",
    "    axes[1, 0].plot(history['train_emotion_loss'], label='Train Emotion', linewidth=2)\n",
    "    axes[1, 0].plot(history['val_emotion_loss'], label='Val Emotion', linewidth=2)\n",
    "    axes[1, 0].set_title('Emotion Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Valence MAE\n",
    "    axes[1, 1].plot(history['train_valence_mae'], label='Train', linewidth=2)\n",
    "    axes[1, 1].plot(history['val_valence_mae'], label='Val', linewidth=2)\n",
    "    axes[1, 1].set_title('Valence MAE')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('MAE')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Arousal MAE\n",
    "    axes[1, 2].plot(history['train_arousal_mae'], label='Train', linewidth=2)\n",
    "    axes[1, 2].plot(history['val_arousal_mae'], label='Val', linewidth=2)\n",
    "    axes[1, 2].set_title('Arousal MAE')\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('MAE')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# ============================================\n",
    "# DATASET\n",
    "# ========================\n",
    "# ============================================\n",
    "# IMPROVED DATASET WITH BETTER ERROR HANDLING\n",
    "# ============================================\n",
    "class EMOTICDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_arrays_dir, image_size=256, transform=None, max_samples=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_arrays_dir = Path(img_arrays_dir)\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Clean data\n",
    "        self.data = self.data.dropna(subset=['Crop_name', 'Valence_norm', 'Arousal_norm']).reset_index(drop=True)\n",
    "        \n",
    "        # Extract dominant emotion\n",
    "        if 'dominant_emotion' not in self.data.columns:\n",
    "            print(\"Extracting dominant_emotion from one-hot encoded columns...\")\n",
    "            emotion_cols = [col for col in EMOTION_COLUMNS if col in self.data.columns]\n",
    "            \n",
    "            if len(emotion_cols) == 0:\n",
    "                raise ValueError(f\"‚ùå No emotion columns found!\")\n",
    "            \n",
    "            emotion_values = self.data[emotion_cols].values\n",
    "            self.data['dominant_emotion'] = emotion_values.argmax(axis=1)\n",
    "        \n",
    "        if max_samples:\n",
    "            self.data = self.data.head(max_samples)\n",
    "        \n",
    "        print(f\"\\n‚úì Dataset loaded: {len(self.data)} samples\")\n",
    "        print(f\"\\nüìä Dominant Emotion Distribution:\")\n",
    "        \n",
    "        self.emotion_counts = np.zeros(26)\n",
    "        for idx in range(len(self.data)):\n",
    "            self.emotion_counts[self.data.iloc[idx]['dominant_emotion']] += 1\n",
    "        \n",
    "        self.present_classes = []\n",
    "        for idx in range(26):\n",
    "            count = int(self.emotion_counts[idx])\n",
    "            if count > 0:\n",
    "                self.present_classes.append(idx)\n",
    "                print(f\"  {idx:2d}. {EMOTION_CLASSES[idx]:20s}: {count:5d} samples ({100*count/len(self.data):5.1f}%)\")\n",
    "        \n",
    "        # Check imbalance\n",
    "        present_counts = self.emotion_counts[self.emotion_counts > 0]\n",
    "        if len(present_counts) > 0:\n",
    "            max_count = present_counts.max()\n",
    "            min_count = present_counts.min()\n",
    "            imbalance_ratio = max_count / min_count\n",
    "            print(f\"\\n‚öñÔ∏è  Class Imbalance Ratio: {imbalance_ratio:.1f}:1\")\n",
    "            print(f\"   Present classes: {len(self.present_classes)}/26\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        crop_name = row['Crop_name']\n",
    "        img_path = self.img_arrays_dir / crop_name\n",
    "        \n",
    "        try:\n",
    "            img_array = np.load(str(img_path))\n",
    "            \n",
    "            if len(img_array.shape) == 3 and img_array.shape[0] == 3:\n",
    "                img_array = img_array.transpose(1, 2, 0)\n",
    "            \n",
    "            if img_array.max() <= 1.0:\n",
    "                img_array = (img_array * 255).astype(np.uint8)\n",
    "            else:\n",
    "                img_array = img_array.astype(np.uint8)\n",
    "            \n",
    "            image = cv2.resize(img_array, (self.image_size, self.image_size))\n",
    "            image_tensor = torch.FloatTensor(image).permute(2, 0, 1) / 255.0\n",
    "            \n",
    "            if self.transform:\n",
    "                image_tensor = self.transform(image_tensor)\n",
    "            \n",
    "            emotion_class = int(row['dominant_emotion'])\n",
    "            emotion_class = torch.LongTensor([emotion_class])[0]\n",
    "            valence = torch.FloatTensor([float(row['Valence_norm'])])[0]\n",
    "            arousal = torch.FloatTensor([float(row['Arousal_norm'])])[0]\n",
    "            \n",
    "            return {\n",
    "                'image': image_tensor,\n",
    "                'emotion': emotion_class,\n",
    "                'valence': valence,\n",
    "                'arousal': arousal,\n",
    "                'filename': crop_name\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load {crop_name}: {e}\")\n",
    "            dummy_image = torch.zeros(3, self.image_size, self.image_size)\n",
    "            return {\n",
    "                'image': dummy_image,\n",
    "                'emotion': torch.LongTensor([0])[0],\n",
    "                'valence': torch.FloatTensor([0.0])[0],\n",
    "                'arousal': torch.FloatTensor([0.0])[0],\n",
    "                'filename': 'dummy'\n",
    "            }\n",
    "\n",
    "# ============================================\n",
    "# CLASS WEIGHTING WITH EXTREME IMBALANCE HANDLING\n",
    "# ============================================\n",
    "def compute_class_weights(train_dataset, method='effective_number', beta=0.9999):\n",
    "    \"\"\"\n",
    "    Compute class weights with multiple strategies for extreme imbalance\n",
    "    \n",
    "    Args:\n",
    "        train_dataset: Dataset instance\n",
    "        method: 'inverse_freq', 'effective_number', or 'sqrt_inv_freq'\n",
    "        beta: For effective number method (0.9999 works well for extreme imbalance)\n",
    "    \"\"\"\n",
    "    emotion_counts = train_dataset.emotion_counts\n",
    "    total = emotion_counts.sum()\n",
    "    \n",
    "    class_weights = np.zeros(26)\n",
    "    \n",
    "    if method == 'inverse_freq':\n",
    "        # Standard inverse frequency\n",
    "        for i in range(26):\n",
    "            if emotion_counts[i] > 0:\n",
    "                class_weights[i] = total / (26 * emotion_counts[i])\n",
    "    \n",
    "    elif method == 'effective_number':\n",
    "        # Effective number of samples (better for extreme imbalance)\n",
    "        # Reference: \"Class-Balanced Loss Based on Effective Number of Samples\"\n",
    "        for i in range(26):\n",
    "            if emotion_counts[i] > 0:\n",
    "                effective_num = (1.0 - np.power(beta, emotion_counts[i])) / (1.0 - beta)\n",
    "                class_weights[i] = 1.0 / effective_num\n",
    "    \n",
    "    elif method == 'sqrt_inv_freq':\n",
    "        # Square root inverse frequency (softer than inverse)\n",
    "        for i in range(26):\n",
    "            if emotion_counts[i] > 0:\n",
    "                class_weights[i] = np.sqrt(total / emotion_counts[i])\n",
    "    \n",
    "    # Normalize weights\n",
    "    class_weights = class_weights / class_weights.sum() * 26\n",
    "    \n",
    "    # Cap extreme weights\n",
    "    class_weights = np.clip(class_weights, 0, 20.0)\n",
    "    \n",
    "    class_weights = torch.FloatTensor(class_weights)\n",
    "    \n",
    "    print(f\"\\nüìä Class Weights (method={method}):\")\n",
    "    print(f\"{'Emotion':<25} {'Count':<10} {'Weight':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i in range(26):\n",
    "        if emotion_counts[i] > 0:\n",
    "            print(f\"{EMOTION_CLASSES[i]:<25} {int(emotion_counts[i]):<10} {class_weights[i]:.3f}\")\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "def create_balanced_sampler(train_dataset):\n",
    "    \"\"\"\n",
    "    Create weighted sampler for balanced batch sampling\n",
    "    \"\"\"\n",
    "    emotion_counts = train_dataset.emotion_counts\n",
    "    \n",
    "    # Compute sample weights (inverse frequency)\n",
    "    sample_weights = np.zeros(len(train_dataset))\n",
    "    \n",
    "    for idx in range(len(train_dataset)):\n",
    "        emotion_class = train_dataset.data.iloc[idx]['dominant_emotion']\n",
    "        if emotion_counts[emotion_class] > 0:\n",
    "            sample_weights[idx] = 1.0 / emotion_counts[emotion_class]\n",
    "    \n",
    "    # Normalize\n",
    "    sample_weights = sample_weights / sample_weights.sum()\n",
    "    \n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(train_dataset),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úì Created WeightedRandomSampler for balanced training\")\n",
    "    \n",
    "    return sampler\n",
    "# ============================================\n",
    "# ENHANCED METRICS WITH TOP-K ACCURACY\n",
    "# ============================================\n",
    "class EnhancedMetricsTracker:\n",
    "    def __init__(self, num_classes=26, class_names=None, top_k_values=[1, 3, 5]):\n",
    "        self.num_classes = num_classes\n",
    "        self.class_names = class_names or [str(i) for i in range(num_classes)]\n",
    "        self.top_k_values = top_k_values\n",
    "        \n",
    "        self.all_preds = []\n",
    "        self.all_labels = []\n",
    "        self.all_probs = []  # Store full probability distributions\n",
    "        self.all_valence_preds = []\n",
    "        self.all_valence_labels = []\n",
    "        self.all_arousal_preds = []\n",
    "        self.all_arousal_labels = []\n",
    "    \n",
    "    def update(self, probs, labels, valence_preds=None, valence_labels=None, \n",
    "               arousal_preds=None, arousal_labels=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            probs: Softmax probabilities (N, num_classes)\n",
    "            labels: Ground truth labels (N,)\n",
    "        \"\"\"\n",
    "        # Store probabilities\n",
    "        self.all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        # Get predictions (argmax)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        self.all_preds.extend(preds.cpu().numpy())\n",
    "        self.all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        if valence_preds is not None:\n",
    "            self.all_valence_preds.extend(valence_preds.cpu().numpy())\n",
    "            self.all_valence_labels.extend(valence_labels.cpu().numpy())\n",
    "        \n",
    "        if arousal_preds is not None:\n",
    "            self.all_arousal_preds.extend(arousal_preds.cpu().numpy())\n",
    "            self.all_arousal_labels.extend(arousal_labels.cpu().numpy())\n",
    "    \n",
    "    def compute(self):\n",
    "        preds = np.array(self.all_preds)\n",
    "        labels = np.array(self.all_labels)\n",
    "        probs = np.concatenate(self.all_probs, axis=0)  # (N, num_classes)\n",
    "        \n",
    "        # Verify probabilities sum to 1\n",
    "        prob_sums = probs.sum(axis=1)\n",
    "        assert np.allclose(prob_sums, 1.0, atol=1e-5), \\\n",
    "            f\"‚ùå Probabilities don't sum to 1! Range: [{prob_sums.min():.6f}, {prob_sums.max():.6f}]\"\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(labels, preds),\n",
    "            'f1_macro': f1_score(labels, preds, average='macro', zero_division=0),\n",
    "            'f1_weighted': f1_score(labels, preds, average='weighted', zero_division=0),\n",
    "            'precision_macro': precision_score(labels, preds, average='macro', zero_division=0),\n",
    "            'recall_macro': recall_score(labels, preds, average='macro', zero_division=0)\n",
    "        }\n",
    "        \n",
    "        # ‚úÖ TOP-K ACCURACY\n",
    "        for k in self.top_k_values:\n",
    "            top_k_acc = self._compute_top_k_accuracy(probs, labels, k)\n",
    "            metrics[f'top_{k}_accuracy'] = top_k_acc\n",
    "        \n",
    "        # ‚úÖ PROBABILITY ANALYSIS\n",
    "        prob_stats = self._analyze_probabilities(probs, labels)\n",
    "        metrics.update(prob_stats)\n",
    "        \n",
    "        # Regression metrics\n",
    "        if self.all_valence_preds:\n",
    "            valence_preds = np.array(self.all_valence_preds)\n",
    "            valence_labels = np.array(self.all_valence_labels)\n",
    "            metrics['valence_mae'] = mean_absolute_error(valence_labels, valence_preds)\n",
    "            metrics['valence_rmse'] = np.sqrt(mean_squared_error(valence_labels, valence_preds))\n",
    "            metrics['valence_r2'] = r2_score(valence_labels, valence_preds)\n",
    "        \n",
    "        if self.all_arousal_preds:\n",
    "            arousal_preds = np.array(self.all_arousal_preds)\n",
    "            arousal_labels = np.array(self.all_arousal_labels)\n",
    "            metrics['arousal_mae'] = mean_absolute_error(arousal_labels, arousal_preds)\n",
    "            metrics['arousal_rmse'] = np.sqrt(mean_squared_error(arousal_labels, arousal_preds))\n",
    "            metrics['arousal_r2'] = r2_score(arousal_labels, arousal_preds)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _compute_top_k_accuracy(self, probs, labels, k):\n",
    "        \"\"\"\n",
    "        Check if true label is in top-k predictions\n",
    "        \"\"\"\n",
    "        top_k_preds = np.argsort(probs, axis=1)[:, -k:]  # (N, k)\n",
    "        correct = 0\n",
    "        for i, label in enumerate(labels):\n",
    "            if label in top_k_preds[i]:\n",
    "                correct += 1\n",
    "        return correct / len(labels)\n",
    "    \n",
    "    def _analyze_probabilities(self, probs, labels):\n",
    "        \"\"\"\n",
    "        Analyze probability distributions\n",
    "        \"\"\"\n",
    "        # Max probability statistics\n",
    "        max_probs = probs.max(axis=1)\n",
    "        \n",
    "        # Probability of true label\n",
    "        true_label_probs = probs[np.arange(len(labels)), labels]\n",
    "        \n",
    "        # Entropy (measure of uncertainty)\n",
    "        epsilon = 1e-10\n",
    "        entropy = -np.sum(probs * np.log(probs + epsilon), axis=1)\n",
    "        \n",
    "        return {\n",
    "            'mean_max_prob': max_probs.mean(),\n",
    "            'std_max_prob': max_probs.std(),\n",
    "            'mean_true_prob': true_label_probs.mean(),\n",
    "            'std_true_prob': true_label_probs.std(),\n",
    "            'mean_entropy': entropy.mean(),\n",
    "            'std_entropy': entropy.std()\n",
    "        }\n",
    "    \n",
    "    def print_metrics(self, metrics_dict, phase=\"\"):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"{phase} Metrics\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Classification metrics\n",
    "        print(f\"\\nüìä Classification Performance:\")\n",
    "        print(f\"  Top-1 Accuracy:   {metrics_dict['accuracy']*100:.2f}%\")\n",
    "        for k in self.top_k_values:\n",
    "            if f'top_{k}_accuracy' in metrics_dict:\n",
    "                print(f\"  Top-{k} Accuracy:   {metrics_dict[f'top_{k}_accuracy']*100:.2f}%\")\n",
    "        \n",
    "        print(f\"\\n  F1 (Macro):       {metrics_dict['f1_macro']:.4f}\")\n",
    "        print(f\"  F1 (Weighted):    {metrics_dict['f1_weighted']:.4f}\")\n",
    "        print(f\"  Precision:        {metrics_dict['precision_macro']:.4f}\")\n",
    "        print(f\"  Recall:           {metrics_dict['recall_macro']:.4f}\")\n",
    "        \n",
    "        # Probability analysis\n",
    "        print(f\"\\nüìà Probability Analysis:\")\n",
    "        print(f\"  Mean Max Prob:    {metrics_dict['mean_max_prob']:.4f} ¬± {metrics_dict['std_max_prob']:.4f}\")\n",
    "        print(f\"  Mean True Prob:   {metrics_dict['mean_true_prob']:.4f} ¬± {metrics_dict['std_true_prob']:.4f}\")\n",
    "        print(f\"  Mean Entropy:     {metrics_dict['mean_entropy']:.4f} ¬± {metrics_dict['std_entropy']:.4f}\")\n",
    "        \n",
    "        # Regression metrics\n",
    "        if 'valence_mae' in metrics_dict:\n",
    "            print(f\"\\nüìâ Valence Regression:\")\n",
    "            print(f\"  MAE:  {metrics_dict['valence_mae']:.4f}\")\n",
    "            print(f\"  RMSE: {metrics_dict['valence_rmse']:.4f}\")\n",
    "            print(f\"  R¬≤:   {metrics_dict['valence_r2']:.4f}\")\n",
    "        \n",
    "        if 'arousal_mae' in metrics_dict:\n",
    "            print(f\"\\nüìâ Arousal Regression:\")\n",
    "            print(f\"  MAE:  {metrics_dict['arousal_mae']:.4f}\")\n",
    "            print(f\"  RMSE: {metrics_dict['arousal_rmse']:.4f}\")\n",
    "            print(f\"  R¬≤:   {metrics_dict['arousal_r2']:.4f}\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# PROBABILITY VERIFICATION UTILITY\n",
    "# ============================================\n",
    "def verify_probabilities(probs, labels, sample_size=5):\n",
    "    \"\"\"\n",
    "    Detailed probability verification for debugging\n",
    "    \n",
    "    Args:\n",
    "        probs: (N, num_classes) probability tensor\n",
    "        labels: (N,) ground truth labels\n",
    "        sample_size: number of samples to print\n",
    "    \"\"\"\n",
    "    probs_np = probs.detach().cpu().numpy()\n",
    "    labels_np = labels.detach().cpu().numpy()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üîç PROBABILITY VERIFICATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Check normalization\n",
    "    prob_sums = probs_np.sum(axis=1)\n",
    "    print(f\"\\n‚úì Probability Normalization:\")\n",
    "    print(f\"  Min sum: {prob_sums.min():.8f}\")\n",
    "    print(f\"  Max sum: {prob_sums.max():.8f}\")\n",
    "    print(f\"  Mean sum: {prob_sums.mean():.8f}\")\n",
    "    print(f\"  All close to 1.0: {np.allclose(prob_sums, 1.0, atol=1e-5)}\")\n",
    "    \n",
    "    # Sample analysis\n",
    "    print(f\"\\nüìã Sample Analysis (first {sample_size} samples):\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for i in range(min(sample_size, len(labels_np))):\n",
    "        true_label = labels_np[i]\n",
    "        sample_probs = probs_np[i]\n",
    "        \n",
    "        # Get top-5 predictions\n",
    "        top_5_indices = np.argsort(sample_probs)[-5:][::-1]\n",
    "        top_5_probs = sample_probs[top_5_indices]\n",
    "        \n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"  True Label: {true_label} ({EMOTION_CLASSES[true_label]})\")\n",
    "        print(f\"  True Label Prob: {sample_probs[true_label]:.4f}\")\n",
    "        print(f\"  Probability Sum: {sample_probs.sum():.8f}\")\n",
    "        \n",
    "        print(f\"\\n  Top-5 Predictions:\")\n",
    "        for rank, (idx, prob) in enumerate(zip(top_5_indices, top_5_probs), 1):\n",
    "            marker = \"‚úì\" if idx == true_label else \" \"\n",
    "            print(f\"    {rank}. {marker} [{idx:2d}] {EMOTION_CLASSES[idx]:20s}: {prob:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "# ============================================\n",
    "# ENHANCED TRAINING WITH PROBABILITY ANALYSIS\n",
    "# ============================================\n",
    "def train_emonet_with_probability_analysis(config):\n",
    "    \"\"\"\n",
    "    Training with comprehensive probability verification and top-k metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    save_dir = Path(config['save_path'])\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plots_dir = save_dir / 'plots'\n",
    "    plots_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save config\n",
    "    with open(save_dir / 'config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training Configuration\")\n",
    "    print(f\"{'='*70}\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key:20s}: {value}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.05),\n",
    "        transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),\n",
    "    ])\n",
    "    \n",
    "    # Datasets\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = EMOTICDataset(\n",
    "        csv_file=config['train_csv'],\n",
    "        img_arrays_dir=config['img_arrays_dir'],\n",
    "        image_size=config['image_size'],\n",
    "        transform=train_transform,\n",
    "        max_samples=config.get('max_samples')\n",
    "    )\n",
    "    \n",
    "    val_dataset = EMOTICDataset(\n",
    "        csv_file=config['val_csv'],\n",
    "        img_arrays_dir=config['img_arrays_dir'],\n",
    "        image_size=config['image_size'],\n",
    "        transform=None,\n",
    "        max_samples=config.get('max_samples')\n",
    "    )\n",
    "    \n",
    "    # Class weights and balanced sampling\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"HANDLING CLASS IMBALANCE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    class_weights = compute_class_weights(\n",
    "        train_dataset, \n",
    "        method=config.get('weight_method', 'effective_number'),\n",
    "        beta=config.get('weight_beta', 0.9999)\n",
    "    )\n",
    "    \n",
    "    balanced_sampler = create_balanced_sampler(train_dataset)\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        sampler=balanced_sampler,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"MODEL SETUP\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = EmoNetSingleLabel26(n_expression=26, n_reg=2, attention=True).to(device)\n",
    "    \n",
    "    # Load pretrained weights\n",
    "    if config.get('pretrained_path') and os.path.exists(config['pretrained_path']):\n",
    "        model.load_pretrained_emonet(\n",
    "            config['pretrained_path'],\n",
    "            freeze_backbone=False\n",
    "        )\n",
    "    \n",
    "    # Loss with class weights\n",
    "    class_weights = class_weights.to(device)\n",
    "    emotion_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    regression_criterion = nn.MSELoss()\n",
    "    \n",
    "    # Optimizer\n",
    "    backbone_params = []\n",
    "    head_params = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if any(x in name for x in ['emotion_head', 'valence_head', 'arousal_head', 'emo_fc_shared']):\n",
    "            head_params.append(param)\n",
    "        else:\n",
    "            backbone_params.append(param)\n",
    "    \n",
    "    if config.get('pretrained_path'):\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': head_params, 'lr': config['learning_rate']},\n",
    "            {'params': backbone_params, 'lr': config['learning_rate'] * 0.1}\n",
    "        ], weight_decay=config['weight_decay'])\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], \n",
    "                              weight_decay=config['weight_decay'])\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5, verbose=True, min_lr=1e-7\n",
    "    )\n",
    "    \n",
    "    # ‚úÖ Use enhanced metrics tracker\n",
    "    top_k_values = config.get('top_k_values', [1, 3, 5])\n",
    "    \n",
    "    # Training history (now includes top-k accuracies)\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_f1': [], 'val_f1': [],\n",
    "        'train_emotion_loss': [], 'val_emotion_loss': [],\n",
    "        'train_valence_loss': [], 'val_valence_loss': [],\n",
    "        'train_arousal_loss': [], 'val_arousal_loss': [],\n",
    "        'train_valence_mae': [], 'val_valence_mae': [],\n",
    "        'train_arousal_mae': [], 'val_arousal_mae': [],\n",
    "        'learning_rate': [],\n",
    "        'train_mean_max_prob': [], 'val_mean_max_prob': [],\n",
    "        'train_mean_true_prob': [], 'val_mean_true_prob': [],\n",
    "    }\n",
    "    \n",
    "    # Add top-k accuracy tracking\n",
    "    for k in top_k_values:\n",
    "        history[f'train_top_{k}_acc'] = []\n",
    "        history[f'val_top_{k}_acc'] = []\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    best_val_top_5 = 0.0  # Track best top-5 accuracy\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    max_patience = 15\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # ========== TRAINING ==========\n",
    "        model.train()\n",
    "        train_metrics = EnhancedMetricsTracker(\n",
    "            num_classes=26, \n",
    "            class_names=EMOTION_COLUMNS,\n",
    "            top_k_values=top_k_values\n",
    "        )\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_emotion_loss = 0.0\n",
    "        train_valence_loss = 0.0\n",
    "        train_arousal_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            images = batch['image'].to(device)\n",
    "            emotion_labels = batch['emotion'].to(device)\n",
    "            valence_labels = batch['valence'].to(device)\n",
    "            arousal_labels = batch['arousal'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Get probabilities (softmax)\n",
    "            logits = outputs['expression']\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            # ‚úÖ Verify probabilities in first batch of first epoch\n",
    "            if epoch == 0 and batch_idx == 0:\n",
    "                verify_probabilities(probs, emotion_labels, sample_size=5)\n",
    "            \n",
    "            # Losses\n",
    "            e_loss = emotion_criterion(logits, emotion_labels)\n",
    "            v_loss = regression_criterion(outputs['valence'], valence_labels)\n",
    "            a_loss = regression_criterion(outputs['arousal'], arousal_labels)\n",
    "            \n",
    "            total_loss = (\n",
    "                config.get('emotion_loss_weight', 5.0) * e_loss + \n",
    "                config.get('valence_loss_weight', 0.5) * v_loss + \n",
    "                config.get('arousal_loss_weight', 0.5) * a_loss\n",
    "            )\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += total_loss.item()\n",
    "            train_emotion_loss += e_loss.item()\n",
    "            train_valence_loss += v_loss.item()\n",
    "            train_arousal_loss += a_loss.item()\n",
    "            \n",
    "            # ‚úÖ Update metrics with probabilities\n",
    "            train_metrics.update(\n",
    "                probs.detach(), emotion_labels.detach(),\n",
    "                outputs['valence'].detach(), valence_labels.detach(),\n",
    "                outputs['arousal'].detach(), arousal_labels.detach()\n",
    "            )\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{total_loss.item():.4f}',\n",
    "                'E': f'{e_loss.item():.3f}',\n",
    "                'V': f'{v_loss.item():.3f}',\n",
    "                'A': f'{a_loss.item():.3f}'\n",
    "            })\n",
    "        \n",
    "        train_metrics_dict = train_metrics.compute()\n",
    "        \n",
    "        # ========== VALIDATION ==========\n",
    "        model.eval()\n",
    "        val_metrics = EnhancedMetricsTracker(\n",
    "            num_classes=26, \n",
    "            class_names=EMOTION_COLUMNS,\n",
    "            top_k_values=top_k_values\n",
    "        )\n",
    "        \n",
    "        val_loss = 0.0\n",
    "        val_emotion_loss = 0.0\n",
    "        val_valence_loss = 0.0\n",
    "        val_arousal_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\")\n",
    "            for batch_idx, batch in enumerate(pbar):\n",
    "                images = batch['image'].to(device)\n",
    "                emotion_labels = batch['emotion'].to(device)\n",
    "                valence_labels = batch['valence'].to(device)\n",
    "                arousal_labels = batch['arousal'].to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                \n",
    "                logits = outputs['expression']\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                \n",
    "                # ‚úÖ Verify validation probabilities in first batch of first epoch\n",
    "                if epoch == 0 and batch_idx == 0:\n",
    "                    print(\"\\nüîç VALIDATION SET PROBABILITIES:\")\n",
    "                    verify_probabilities(probs, emotion_labels, sample_size=3)\n",
    "                \n",
    "                e_loss = emotion_criterion(logits, emotion_labels)\n",
    "                v_loss = regression_criterion(outputs['valence'], valence_labels)\n",
    "                a_loss = regression_criterion(outputs['arousal'], arousal_labels)\n",
    "                \n",
    "                total_loss = (\n",
    "                    config.get('emotion_loss_weight', 5.0) * e_loss + \n",
    "                    config.get('valence_loss_weight', 0.5) * v_loss + \n",
    "                    config.get('arousal_loss_weight', 0.5) * a_loss\n",
    "                )\n",
    "                \n",
    "                val_loss += total_loss.item()\n",
    "                val_emotion_loss += e_loss.item()\n",
    "                val_valence_loss += v_loss.item()\n",
    "                val_arousal_loss += a_loss.item()\n",
    "                \n",
    "                # ‚úÖ Update metrics with probabilities\n",
    "                val_metrics.update(\n",
    "                    probs.detach(), emotion_labels.detach(),\n",
    "                    outputs['valence'].detach(), valence_labels.detach(),\n",
    "                    outputs['arousal'].detach(), arousal_labels.detach()\n",
    "                )\n",
    "                \n",
    "                pbar.set_postfix({'Loss': f'{total_loss.item():.4f}'})\n",
    "        \n",
    "        val_metrics_dict = val_metrics.compute()\n",
    "        \n",
    "        # Calculate averages\n",
    "        train_loss /= len(train_loader)\n",
    "        train_emotion_loss /= len(train_loader)\n",
    "        train_valence_loss /= len(train_loader)\n",
    "        train_arousal_loss /= len(train_loader)\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_emotion_loss /= len(val_loader)\n",
    "        val_valence_loss /= len(val_loader)\n",
    "        val_arousal_loss /= len(val_loader)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_emotion_loss'].append(train_emotion_loss)\n",
    "        history['train_valence_loss'].append(train_valence_loss)\n",
    "        history['train_arousal_loss'].append(train_arousal_loss)\n",
    "        history['train_acc'].append(train_metrics_dict['accuracy'] * 100)\n",
    "        history['train_f1'].append(train_metrics_dict['f1_macro'])\n",
    "        history['train_valence_mae'].append(train_metrics_dict['valence_mae'])\n",
    "        history['train_arousal_mae'].append(train_metrics_dict['arousal_mae'])\n",
    "        history['train_mean_max_prob'].append(train_metrics_dict['mean_max_prob'])\n",
    "        history['train_mean_true_prob'].append(train_metrics_dict['mean_true_prob'])\n",
    "        \n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_emotion_loss'].append(val_emotion_loss)\n",
    "        history['val_valence_loss'].append(val_valence_loss)\n",
    "        history['val_arousal_loss'].append(val_arousal_loss)\n",
    "        history['val_acc'].append(val_metrics_dict['accuracy'] * 100)\n",
    "        history['val_f1'].append(val_metrics_dict['f1_macro'])\n",
    "        history['val_valence_mae'].append(val_metrics_dict['valence_mae'])\n",
    "        history['val_arousal_mae'].append(val_metrics_dict['arousal_mae'])\n",
    "        history['val_mean_max_prob'].append(val_metrics_dict['mean_max_prob'])\n",
    "        history['val_mean_true_prob'].append(val_metrics_dict['mean_true_prob'])\n",
    "        history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # ‚úÖ Track top-k accuracies\n",
    "        for k in top_k_values:\n",
    "            history[f'train_top_{k}_acc'].append(train_metrics_dict[f'top_{k}_accuracy'] * 100)\n",
    "            history[f'val_top_{k}_acc'].append(val_metrics_dict[f'top_{k}_accuracy'] * 100)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train - Loss: {train_loss:.4f} | Top-1: {train_metrics_dict['accuracy']*100:.2f}% | F1: {train_metrics_dict['f1_macro']:.4f}\")\n",
    "        print(f\"          Top-5: {train_metrics_dict['top_5_accuracy']*100:.2f}% | Max Prob: {train_metrics_dict['mean_max_prob']:.3f}\")\n",
    "        print(f\"  Val   - Loss: {val_loss:.4f} | Top-1: {val_metrics_dict['accuracy']*100:.2f}% | F1: {val_metrics_dict['f1_macro']:.4f}\")\n",
    "        print(f\"          Top-5: {val_metrics_dict['top_5_accuracy']*100:.2f}% | Max Prob: {val_metrics_dict['mean_max_prob']:.3f}\")\n",
    "        \n",
    "        # Detailed metrics every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            train_metrics.print_metrics(train_metrics_dict, \"Training\")\n",
    "            val_metrics.print_metrics(val_metrics_dict, \"Validation\")\n",
    "        \n",
    "        # ‚úÖ Save best model based on BOTH F1 and Top-5 accuracy\n",
    "        improvement = False\n",
    "        \n",
    "        # Primary metric: F1 score\n",
    "        if val_metrics_dict['f1_macro'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics_dict['f1_macro']\n",
    "            improvement = True\n",
    "        \n",
    "        # Secondary metric: Top-5 accuracy\n",
    "        if val_metrics_dict['top_5_accuracy'] > best_val_top_5:\n",
    "            best_val_top_5 = val_metrics_dict['top_5_accuracy']\n",
    "            improvement = True\n",
    "        \n",
    "        if improvement:\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'config': config,\n",
    "                'history': history,\n",
    "                'best_val_f1': best_val_f1,\n",
    "                'best_val_top_5': best_val_top_5,\n",
    "                'val_metrics': val_metrics_dict,\n",
    "                'class_weights': class_weights.cpu()\n",
    "            }, save_dir / 'best_model.pth')\n",
    "            \n",
    "            print(f\"\\n  ‚úì Saved best model (F1: {best_val_f1:.4f} | Top-5: {best_val_top_5*100:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"\\n  ‚ö† No improvement ({patience_counter}/{max_patience})\")\n",
    "            print(f\"     Best F1: {best_val_f1:.4f} | Best Top-5: {best_val_top_5*100:.2f}%\")\n",
    "        \n",
    " \n",
    "        # Checkpoint\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history\n",
    "            }, save_dir / f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_metrics_dict['f1_macro'])\n",
    "        \n",
    "        # Plot progress\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            plot_training_progress_enhanced(history, plots_dir / f'progress_epoch_{epoch+1}.png', top_k_values)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Best Epoch: {best_epoch}\")\n",
    "    print(f\"Best Val F1: {best_val_f1:.4f}\")\n",
    "    print(f\"Best Val Top-5: {best_val_top_5*100:.2f}%\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# ============================================\n",
    "# ENHANCED PLOTTING\n",
    "# ============================================\n",
    "def plot_training_progress_enhanced(history, save_path, top_k_values):\n",
    "    \"\"\"Plot with top-k accuracies and probability analysis\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    fig.suptitle('Training Progress - Enhanced Metrics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Loss\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(history['train_loss'], label='Train', linewidth=2)\n",
    "    ax1.plot(history['val_loss'], label='Val', linewidth=2)\n",
    "    ax1.set_title('Total Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Top-1 Accuracy\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(history['train_acc'], label='Train', linewidth=2)\n",
    "    ax2.plot(history['val_acc'], label='Val', linewidth=2)\n",
    "    ax2.set_title('Top-1 Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ‚úÖ Top-K Accuracies\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    colors = ['blue', 'green', 'orange']\n",
    "    for i, k in enumerate(top_k_values):\n",
    "        if f'val_top_{k}_acc' in history:\n",
    "            ax3.plot(history[f'val_top_{k}_acc'], \n",
    "                    label=f'Top-{k}', linewidth=2, color=colors[i % len(colors)])\n",
    "    ax3.set_title('Top-K Validation Accuracy')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Accuracy (%)')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # F1 Score\n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    ax4.plot(history['train_f1'], label='Train', linewidth=2)\n",
    "    ax4.plot(history['val_f1'], label='Val', linewidth=2)\n",
    "    ax4.set_title('F1 Score (Macro)')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('F1 Score')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # ‚úÖ Probability Analysis\n",
    "    ax5 = fig.add_subplot(gs[1, 0])\n",
    "    ax5.plot(history['train_mean_max_prob'], label='Train', linewidth=2)\n",
    "    ax5.plot(history['val_mean_max_prob'], label='Val', linewidth=2)\n",
    "    ax5.set_title('Mean Max Probability')\n",
    "    ax5.set_xlabel('Epoch')\n",
    "    ax5.set_ylabel('Probability')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax6 = fig.add_subplot(gs[1, 1])\n",
    "    ax6.plot(history['train_mean_true_prob'], label='Train', linewidth=2)\n",
    "    ax6.plot(history['val_mean_true_prob'], label='Val', linewidth=2)\n",
    "    ax6.set_title('Mean True Label Probability')\n",
    "    ax6.set_xlabel('Epoch')\n",
    "    ax6.set_ylabel('Probability')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Component Losses\n",
    "    ax7 = fig.add_subplot(gs[1, 2])\n",
    "    ax7.plot(history['train_emotion_loss'], label='Train', linewidth=2)\n",
    "    ax7.plot(history['val_emotion_loss'], label='Val', linewidth=2)\n",
    "    ax7.set_title('Emotion Loss')\n",
    "    ax7.set_xlabel('Epoch')\n",
    "    ax7.set_ylabel('Loss')\n",
    "    ax7.legend()\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning Rate\n",
    "    ax8 = fig.add_subplot(gs[1, 3])\n",
    "    ax8.plot(history['learning_rate'], linewidth=2, color='red')\n",
    "    ax8.set_title('Learning Rate')\n",
    "    ax8.set_xlabel('Epoch')\n",
    "    ax8.set_ylabel('LR')\n",
    "    ax8.set_yscale('log')\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Valence MAE\n",
    "    ax9 = fig.add_subplot(gs[2, 0])\n",
    "    ax9.plot(history['train_valence_mae'], label='Train', linewidth=2)\n",
    "    ax9.plot(history['val_valence_mae'], label='Val', linewidth=2)\n",
    "    ax9.set_title('Valence MAE')\n",
    "    ax9.set_xlabel('Epoch')\n",
    "    ax9.set_ylabel('MAE')\n",
    "    ax9.legend()\n",
    "    ax9.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Arousal MAE\n",
    "    ax10 = fig.add_subplot(gs[2, 1])\n",
    "    ax10.plot(history['train_arousal_mae'], label='Train', linewidth=2)\n",
    "    ax10.plot(history['val_arousal_mae'], label='Val', linewidth=2)\n",
    "    ax10.set_title('Arousal MAE')\n",
    "    ax10.set_xlabel('Epoch')\n",
    "    ax10.set_ylabel('MAE')\n",
    "    ax10.legend()\n",
    "    ax10.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Valence Loss\n",
    "    ax11 = fig.add_subplot(gs[2, 2])\n",
    "    ax11.plot(history['train_valence_loss'], label='Train', linewidth=2)\n",
    "    ax11.plot(history['val_valence_loss'], label='Val', linewidth=2)\n",
    "    ax11.set_title('Valence Loss')\n",
    "    ax11.set_xlabel('Epoch')\n",
    "    ax11.set_ylabel('Loss')\n",
    "    ax11.legend()\n",
    "    ax11.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Arousal Loss\n",
    "    ax12 = fig.add_subplot(gs[2, 3])\n",
    "    ax12.plot(history['train_arousal_loss'], label='Train', linewidth=2)\n",
    "    ax12.plot(history['val_arousal_loss'], label='Val', linewidth=2)\n",
    "    ax12.set_title('Arousal Loss')\n",
    "    ax12.set_xlabel('Epoch')\n",
    "    ax12.set_ylabel('Loss')\n",
    "    ax12.legend()\n",
    "    ax12.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T18:55:37.525047Z",
     "iopub.status.busy": "2025-10-23T18:55:37.524746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Training Configuration\n",
      "======================================================================\n",
      "  train_csv           : /kaggle/input/emotic/normalized_balanced_limited_dominant_emotion.csv\n",
      "  val_csv             : /kaggle/input/emotic/normalized_dataset_val_dominant_emotion.csv\n",
      "  img_arrays_dir      : /kaggle/input/emotic/archive_emot 2/archive_emot/img_arrs\n",
      "  save_path           : /kaggle/working/\n",
      "  pretrained_path     : /kaggle/input/emotic/emonet_8.pth\n",
      "  freeze_backbone     : False\n",
      "  image_size          : 256\n",
      "  num_epochs          : 100\n",
      "  batch_size          : 64\n",
      "  learning_rate       : 0.0001\n",
      "  weight_decay        : 0.0001\n",
      "  num_workers         : 4\n",
      "  emotion_loss_weight : 2.0\n",
      "  valence_loss_weight : 0.5\n",
      "  arousal_loss_weight : 0.5\n",
      "  weight_method       : effective_number\n",
      "  weight_beta         : 0.9999\n",
      "  top_k_values        : [1, 3, 5]\n",
      "  max_samples         : None\n",
      "======================================================================\n",
      "\n",
      "Loading datasets...\n",
      "Extracting dominant_emotion from one-hot encoded columns...\n",
      "\n",
      "‚úì Dataset loaded: 3862 samples\n",
      "\n",
      "üìä Dominant Emotion Distribution:\n",
      "   0. Peace               :   441 samples ( 11.4%)\n",
      "   1. Affection           :   360 samples (  9.3%)\n",
      "   2. Esteem              :   224 samples (  5.8%)\n",
      "   3. Anticipation        :   528 samples ( 13.7%)\n",
      "   4. Engagement          :   445 samples ( 11.5%)\n",
      "   5. Confidence          :   213 samples (  5.5%)\n",
      "   6. Happiness           :   348 samples (  9.0%)\n",
      "   7. Pleasure            :    45 samples (  1.2%)\n",
      "   8. Excitement          :    46 samples (  1.2%)\n",
      "   9. Surprise            :    98 samples (  2.5%)\n",
      "  10. Sympathy            :    62 samples (  1.6%)\n",
      "  11. Doubt/Confusion     :   147 samples (  3.8%)\n",
      "  12. Disconnection       :   197 samples (  5.1%)\n",
      "  13. Fatigue             :    96 samples (  2.5%)\n",
      "  14. Embarrassment       :    33 samples (  0.9%)\n",
      "  15. Yearning            :   169 samples (  4.4%)\n",
      "  16. Disapproval         :    86 samples (  2.2%)\n",
      "  17. Aversion            :    23 samples (  0.6%)\n",
      "  18. Annoyance           :    55 samples (  1.4%)\n",
      "  19. Anger               :    40 samples (  1.0%)\n",
      "  20. Sensitivity         :    68 samples (  1.8%)\n",
      "  21. Sadness             :    84 samples (  2.2%)\n",
      "  22. Disquietment        :    17 samples (  0.4%)\n",
      "  23. Fear                :    16 samples (  0.4%)\n",
      "  24. Pain                :    13 samples (  0.3%)\n",
      "  25. Suffering           :     8 samples (  0.2%)\n",
      "\n",
      "‚öñÔ∏è  Class Imbalance Ratio: 66.0:1\n",
      "   Present classes: 26/26\n",
      "Extracting dominant_emotion from one-hot encoded columns...\n",
      "\n",
      "‚úì Dataset loaded: 958 samples\n",
      "\n",
      "üìä Dominant Emotion Distribution:\n",
      "   0. Peace               :   236 samples ( 24.6%)\n",
      "   1. Affection           :   182 samples ( 19.0%)\n",
      "   2. Esteem              :    99 samples ( 10.3%)\n",
      "   3. Anticipation        :   375 samples ( 39.1%)\n",
      "   4. Engagement          :    30 samples (  3.1%)\n",
      "   6. Happiness           :     1 samples (  0.1%)\n",
      "   7. Pleasure            :     1 samples (  0.1%)\n",
      "   8. Excitement          :     3 samples (  0.3%)\n",
      "   9. Surprise            :     4 samples (  0.4%)\n",
      "  10. Sympathy            :     3 samples (  0.3%)\n",
      "  11. Doubt/Confusion     :    10 samples (  1.0%)\n",
      "  12. Disconnection       :     7 samples (  0.7%)\n",
      "  13. Fatigue             :     3 samples (  0.3%)\n",
      "  16. Disapproval         :     2 samples (  0.2%)\n",
      "  18. Annoyance           :     1 samples (  0.1%)\n",
      "  21. Sadness             :     1 samples (  0.1%)\n",
      "\n",
      "‚öñÔ∏è  Class Imbalance Ratio: 375.0:1\n",
      "   Present classes: 16/26\n",
      "\n",
      "======================================================================\n",
      "HANDLING CLASS IMBALANCE\n",
      "======================================================================\n",
      "\n",
      "üìä Class Weights (method=effective_number):\n",
      "Emotion                   Count      Weight    \n",
      "--------------------------------------------------\n",
      "Peace                     441        0.101\n",
      "Affection                 360        0.123\n",
      "Esteem                    224        0.196\n",
      "Anticipation              528        0.084\n",
      "Engagement                445        0.100\n",
      "Confidence                213        0.206\n",
      "Happiness                 348        0.127\n",
      "Pleasure                  45         0.966\n",
      "Excitement                46         0.945\n",
      "Surprise                  98         0.445\n",
      "Sympathy                  62         0.702\n",
      "Doubt/Confusion           147        0.297\n",
      "Disconnection             197        0.222\n",
      "Fatigue                   96         0.454\n",
      "Embarrassment             33         1.316\n",
      "Yearning                  169        0.259\n",
      "Disapproval               86         0.506\n",
      "Aversion                  23         1.888\n",
      "Annoyance                 55         0.791\n",
      "Anger                     40         1.086\n",
      "Sensitivity               68         0.640\n",
      "Sadness                   84         0.518\n",
      "Disquietment              17         2.553\n",
      "Fear                      16         2.713\n",
      "Pain                      13         3.338\n",
      "Suffering                 8          5.424\n",
      "\n",
      "‚úì Created WeightedRandomSampler for balanced training\n",
      "\n",
      "======================================================================\n",
      "MODEL SETUP\n",
      "======================================================================\n",
      "Using device: cuda\n",
      "Loading pretrained EmoNet from /kaggle/input/emotic/emonet_8.pth\n",
      "‚úì Loaded 673 layers from pretrained model\n",
      "‚úó Skipped 9 incompatible layers\n",
      "\n",
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Epoch 1/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîç PROBABILITY VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "‚úì Probability Normalization:\n",
      "  Min sum: 0.99999982\n",
      "  Max sum: 1.00000012\n",
      "  Mean sum: 1.00000000\n",
      "  All close to 1.0: True\n",
      "\n",
      "üìã Sample Analysis (first 5 samples):\n",
      "======================================================================\n",
      "\n",
      "Sample 1:\n",
      "  True Label: 21 (Sadness)\n",
      "  True Label Prob: 0.1000\n",
      "  Probability Sum: 0.99999994\n",
      "\n",
      "  Top-5 Predictions:\n",
      "    1.   [ 7] Pleasure            : 0.1502\n",
      "    2.   [ 4] Engagement          : 0.1051\n",
      "    3. ‚úì [21] Sadness             : 0.1000\n",
      "    4.   [ 0] Peace               : 0.0923\n",
      "    5.   [19] Anger               : 0.0870\n",
      "\n",
      "Sample 2:\n",
      "  True Label: 8 (Excitement)\n",
      "  True Label Prob: 0.0190\n",
      "  Probability Sum: 1.00000012\n",
      "\n",
      "  Top-5 Predictions:\n",
      "    1.   [ 3] Anticipation        : 0.1430\n",
      "    2.   [19] Anger               : 0.0851\n",
      "    3.   [20] Sensitivity         : 0.0807\n",
      "    4.   [24] Pain                : 0.0557\n",
      "    5.   [ 1] Affection           : 0.0545\n",
      "\n",
      "Sample 3:\n",
      "  True Label: 18 (Annoyance)\n",
      "  True Label Prob: 0.0393\n",
      "  Probability Sum: 1.00000000\n",
      "\n",
      "  Top-5 Predictions:\n",
      "    1.   [24] Pain                : 0.0906\n",
      "    2.   [ 0] Peace               : 0.0737\n",
      "    3.   [11] Doubt/Confusion     : 0.0560\n",
      "    4.   [17] Aversion            : 0.0557\n",
      "    5.   [ 3] Anticipation        : 0.0541\n",
      "\n",
      "Sample 4:\n",
      "  True Label: 4 (Engagement)\n",
      "  True Label Prob: 0.0264\n",
      "  Probability Sum: 0.99999994\n",
      "\n",
      "  Top-5 Predictions:\n",
      "    1.   [ 3] Anticipation        : 0.0680\n",
      "    2.   [19] Anger               : 0.0653\n",
      "    3.   [21] Sadness             : 0.0600\n",
      "    4.   [11] Doubt/Confusion     : 0.0529\n",
      "    5.   [16] Disapproval         : 0.0493\n",
      "\n",
      "Sample 5:\n",
      "  True Label: 12 (Disconnection)\n",
      "  True Label Prob: 0.0307\n",
      "  Probability Sum: 1.00000000\n",
      "\n",
      "  Top-5 Predictions:\n",
      "    1.   [ 0] Peace               : 0.0831\n",
      "    2.   [11] Doubt/Confusion     : 0.0760\n",
      "    3.   [20] Sensitivity         : 0.0684\n",
      "    4.   [ 1] Affection           : 0.0543\n",
      "    5.   [16] Disapproval         : 0.0498\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:58<00:00,  1.03it/s, Loss=6.6414, E=3.199, V=0.201, A=0.286]\n",
      "Validation Epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç VALIDATION SET PROBABILITIES:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 1:   7%|‚ñã         | 1/15 [00:01<00:16,  1.20s/it, Loss=6.6510]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîç PROBABILITY VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "‚úì Probability Normalization:\n",
      "  Min sum: 0.99999988\n",
      "  Max sum: 1.00000012\n",
      "  Mean sum: 1.00000000\n",
      "  All close to 1.0: True\n",
      "\n",
      "üìã Sample Analysis (first 3 samples):\n",
      "======================================================================\n",
      "\n",
      "Sample 1:\n",
      "  True Label: 4 (Engagement)\n",
      "  True Label Prob: 0.0157\n",
      "  Probability Sum: 1.00000000\n",
      "\n",
      "  Top-5 Predictions:\n",
      "    1.   [24] Pain                : 0.0889\n",
      "    2.   [11] Doubt/Confusion     : 0.0685\n",
      "    3.   [25] Suffering           : 0.0612\n",
      "    4.   [20] Sensitivity         : 0.0555\n",
      "    5.   [ 3] Anticipation        : 0.0555\n",
      "\n",
      "Sample 2:\n",
      "  True Label: 3 (Anticipation)\n",
      "  True Label Prob: 0.0291\n",
      "  Probability Sum: 1.00000012\n",
      "\n",
      "  Top-5 Predictions:\n",
      "    1.   [ 2] Esteem              : 0.0621\n",
      "    2.   [17] Aversion            : 0.0558\n",
      "    3.   [24] Pain                : 0.0532\n",
      "    4.   [22] Disquietment        : 0.0509\n",
      "    5.   [11] Doubt/Confusion     : 0.0493\n",
      "\n",
      "Sample 3:\n",
      "  True Label: 1 (Affection)\n",
      "  True Label Prob: 0.0249\n",
      "  Probability Sum: 0.99999994\n",
      "\n",
      "  Top-5 Predictions:\n",
      "    1.   [25] Suffering           : 0.0800\n",
      "    2.   [24] Pain                : 0.0615\n",
      "    3.   [19] Anger               : 0.0573\n",
      "    4.   [11] Doubt/Confusion     : 0.0534\n",
      "    5.   [ 2] Esteem              : 0.0474\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:05<00:00,  2.79it/s, Loss=6.5957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 1 Summary:\n",
      "  Train - Loss: 6.8717 | Top-1: 4.32% | F1: 0.0305\n",
      "          Top-5: 20.76% | Max Prob: 0.089\n",
      "  Val   - Loss: 6.7733 | Top-1: 5.22% | F1: 0.0124\n",
      "          Top-5: 26.51% | Max Prob: 0.081\n",
      "\n",
      "  ‚úì Saved best model (F1: 0.0124 | Top-5: 26.51%)\n",
      "\n",
      "======================================================================\n",
      "Epoch 2/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:55<00:00,  1.08it/s, Loss=5.2369, E=2.483, V=0.228, A=0.313]\n",
      "Validation Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:05<00:00,  3.00it/s, Loss=6.8329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 2 Summary:\n",
      "  Train - Loss: 6.1820 | Top-1: 7.53% | F1: 0.0470\n",
      "          Top-5: 24.56% | Max Prob: 0.099\n",
      "  Val   - Loss: 7.0970 | Top-1: 0.42% | F1: 0.0009\n",
      "          Top-5: 15.66% | Max Prob: 0.108\n",
      "\n",
      "  ‚ö† No improvement (1/15)\n",
      "     Best F1: 0.0124 | Best Top-5: 26.51%\n",
      "\n",
      "======================================================================\n",
      "Epoch 3/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:55<00:00,  1.08it/s, Loss=6.0674, E=2.949, V=0.194, A=0.146]\n",
      "Validation Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:05<00:00,  3.00it/s, Loss=7.0414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 3 Summary:\n",
      "  Train - Loss: 5.7740 | Top-1: 9.84% | F1: 0.0485\n",
      "          Top-5: 28.49% | Max Prob: 0.125\n",
      "  Val   - Loss: 7.3911 | Top-1: 0.00% | F1: 0.0000\n",
      "          Top-5: 7.31% | Max Prob: 0.139\n",
      "\n",
      "  ‚ö† No improvement (2/15)\n",
      "     Best F1: 0.0124 | Best Top-5: 26.51%\n",
      "\n",
      "======================================================================\n",
      "Epoch 4/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:55<00:00,  1.08it/s, Loss=5.4383, E=2.595, V=0.205, A=0.290]\n",
      "Validation Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:04<00:00,  3.02it/s, Loss=7.3470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 4 Summary:\n",
      "  Train - Loss: 5.4543 | Top-1: 10.89% | F1: 0.0493\n",
      "          Top-5: 28.52% | Max Prob: 0.152\n",
      "  Val   - Loss: 7.7263 | Top-1: 0.00% | F1: 0.0000\n",
      "          Top-5: 2.51% | Max Prob: 0.173\n",
      "\n",
      "  ‚ö† No improvement (3/15)\n",
      "     Best F1: 0.0124 | Best Top-5: 26.51%\n",
      "\n",
      "======================================================================\n",
      "Epoch 5/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:55<00:00,  1.08it/s, Loss=4.8536, E=2.304, V=0.239, A=0.252]\n",
      "Validation Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:05<00:00,  3.00it/s, Loss=7.5909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 5 Summary:\n",
      "  Train - Loss: 5.1214 | Top-1: 12.73% | F1: 0.0540\n",
      "          Top-5: 29.51% | Max Prob: 0.178\n",
      "  Val   - Loss: 7.9928 | Top-1: 0.00% | F1: 0.0000\n",
      "          Top-5: 0.94% | Max Prob: 0.193\n",
      "\n",
      "======================================================================\n",
      "Training Metrics\n",
      "======================================================================\n",
      "\n",
      "üìä Classification Performance:\n",
      "  Top-1 Accuracy:   12.73%\n",
      "  Top-1 Accuracy:   12.73%\n",
      "  Top-3 Accuracy:   22.06%\n",
      "  Top-5 Accuracy:   29.51%\n",
      "\n",
      "  F1 (Macro):       0.0540\n",
      "  F1 (Weighted):    0.0558\n",
      "  Precision:        0.0830\n",
      "  Recall:           0.1262\n",
      "\n",
      "üìà Probability Analysis:\n",
      "  Mean Max Prob:    0.1779 ¬± 0.1063\n",
      "  Mean True Prob:   0.0634 ¬± 0.0922\n",
      "  Mean Entropy:     2.9286 ¬± 0.3070\n",
      "\n",
      "üìâ Valence Regression:\n",
      "  MAE:  0.3549\n",
      "  RMSE: 0.4500\n",
      "  R¬≤:   -0.1668\n",
      "\n",
      "üìâ Arousal Regression:\n",
      "  MAE:  0.3929\n",
      "  RMSE: 0.4890\n",
      "  R¬≤:   -0.2677\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Validation Metrics\n",
      "======================================================================\n",
      "\n",
      "üìä Classification Performance:\n",
      "  Top-1 Accuracy:   0.00%\n",
      "  Top-1 Accuracy:   0.00%\n",
      "  Top-3 Accuracy:   0.21%\n",
      "  Top-5 Accuracy:   0.94%\n",
      "\n",
      "  F1 (Macro):       0.0000\n",
      "  F1 (Weighted):    0.0000\n",
      "  Precision:        0.0000\n",
      "  Recall:           0.0000\n",
      "\n",
      "üìà Probability Analysis:\n",
      "  Mean Max Prob:    0.1930 ¬± 0.1084\n",
      "  Mean True Prob:   0.0218 ¬± 0.0094\n",
      "  Mean Entropy:     2.9153 ¬± 0.2930\n",
      "\n",
      "üìâ Valence Regression:\n",
      "  MAE:  0.3007\n",
      "  RMSE: 0.3804\n",
      "  R¬≤:   -0.9400\n",
      "\n",
      "üìâ Arousal Regression:\n",
      "  MAE:  0.2523\n",
      "  RMSE: 0.3150\n",
      "  R¬≤:   -0.6033\n",
      "======================================================================\n",
      "\n",
      "\n",
      "  ‚ö† No improvement (4/15)\n",
      "     Best F1: 0.0124 | Best Top-5: 26.51%\n",
      "\n",
      "======================================================================\n",
      "Epoch 6/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:55<00:00,  1.08it/s, Loss=5.8683, E=2.835, V=0.169, A=0.228]\n",
      "Validation Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:04<00:00,  3.02it/s, Loss=7.9861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 6 Summary:\n",
      "  Train - Loss: 4.8910 | Top-1: 12.45% | F1: 0.0512\n",
      "          Top-5: 29.35% | Max Prob: 0.206\n",
      "  Val   - Loss: 8.3615 | Top-1: 0.00% | F1: 0.0000\n",
      "          Top-5: 0.31% | Max Prob: 0.238\n",
      "\n",
      "  ‚ö† No improvement (5/15)\n",
      "     Best F1: 0.0124 | Best Top-5: 26.51%\n",
      "\n",
      "======================================================================\n",
      "Epoch 7/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:55<00:00,  1.08it/s, Loss=3.9352, E=1.855, V=0.198, A=0.253]\n",
      "Validation Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:05<00:00,  2.98it/s, Loss=8.1228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 7 Summary:\n",
      "  Train - Loss: 4.7378 | Top-1: 13.57% | F1: 0.0573\n",
      "          Top-5: 30.13% | Max Prob: 0.230\n",
      "  Val   - Loss: 8.5012 | Top-1: 0.00% | F1: 0.0000\n",
      "          Top-5: 0.21% | Max Prob: 0.237\n",
      "\n",
      "  ‚ö† No improvement (6/15)\n",
      "     Best F1: 0.0124 | Best Top-5: 26.51%\n",
      "\n",
      "======================================================================\n",
      "Epoch 8/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:55<00:00,  1.08it/s, Loss=4.9817, E=2.383, V=0.227, A=0.205]\n",
      "Validation Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:05<00:00,  2.99it/s, Loss=8.2963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 8 Summary:\n",
      "  Train - Loss: 4.6865 | Top-1: 13.28% | F1: 0.0550\n",
      "          Top-5: 29.14% | Max Prob: 0.244\n",
      "  Val   - Loss: 8.7166 | Top-1: 0.00% | F1: 0.0000\n",
      "          Top-5: 0.21% | Max Prob: 0.258\n",
      "\n",
      "  ‚ö† No improvement (7/15)\n",
      "     Best F1: 0.0124 | Best Top-5: 26.51%\n",
      "\n",
      "======================================================================\n",
      "Epoch 9/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:55<00:00,  1.08it/s, Loss=4.1729, E=1.967, V=0.197, A=0.280]\n",
      "Validation Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:04<00:00,  3.05it/s, Loss=8.3119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 9 Summary:\n",
      "  Train - Loss: 4.4477 | Top-1: 14.45% | F1: 0.0577\n",
      "          Top-5: 31.41% | Max Prob: 0.254\n",
      "  Val   - Loss: 8.7408 | Top-1: 0.00% | F1: 0.0000\n",
      "          Top-5: 0.21% | Max Prob: 0.258\n",
      "\n",
      "  ‚ö† No improvement (8/15)\n",
      "     Best F1: 0.0124 | Best Top-5: 26.51%\n",
      "\n",
      "======================================================================\n",
      "Epoch 10/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:55<00:00,  1.08it/s, Loss=4.8531, E=2.330, V=0.200, A=0.188]\n",
      "Validation Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:04<00:00,  3.04it/s, Loss=8.4026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 10 Summary:\n",
      "  Train - Loss: 4.5468 | Top-1: 13.96% | F1: 0.0593\n",
      "          Top-5: 29.90% | Max Prob: 0.262\n",
      "  Val   - Loss: 8.8624 | Top-1: 0.00% | F1: 0.0000\n",
      "          Top-5: 0.21% | Max Prob: 0.261\n",
      "\n",
      "======================================================================\n",
      "Training Metrics\n",
      "======================================================================\n",
      "\n",
      "üìä Classification Performance:\n",
      "  Top-1 Accuracy:   13.96%\n",
      "  Top-1 Accuracy:   13.96%\n",
      "  Top-3 Accuracy:   22.19%\n",
      "  Top-5 Accuracy:   29.90%\n",
      "\n",
      "  F1 (Macro):       0.0593\n",
      "  F1 (Weighted):    0.0591\n",
      "  Precision:        0.0943\n",
      "  Recall:           0.1431\n",
      "\n",
      "üìà Probability Analysis:\n",
      "  Mean Max Prob:    0.2619 ¬± 0.1595\n",
      "  Mean True Prob:   0.0837 ¬± 0.1499\n",
      "  Mean Entropy:     2.6677 ¬± 0.4636\n",
      "\n",
      "üìâ Valence Regression:\n",
      "  MAE:  0.3398\n",
      "  RMSE: 0.4308\n",
      "  R¬≤:   -0.0940\n",
      "\n",
      "üìâ Arousal Regression:\n",
      "  MAE:  0.3868\n",
      "  RMSE: 0.4798\n",
      "  R¬≤:   -0.2094\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Validation Metrics\n",
      "======================================================================\n",
      "\n",
      "üìä Classification Performance:\n",
      "  Top-1 Accuracy:   0.00%\n",
      "  Top-1 Accuracy:   0.00%\n",
      "  Top-3 Accuracy:   0.00%\n",
      "  Top-5 Accuracy:   0.21%\n",
      "\n",
      "  F1 (Macro):       0.0000\n",
      "  F1 (Weighted):    0.0000\n",
      "  Precision:        0.0000\n",
      "  Recall:           0.0000\n",
      "\n",
      "üìà Probability Analysis:\n",
      "  Mean Max Prob:    0.2606 ¬± 0.1543\n",
      "  Mean True Prob:   0.0142 ¬± 0.0076\n",
      "  Mean Entropy:     2.6822 ¬± 0.4454\n",
      "\n",
      "üìâ Valence Regression:\n",
      "  MAE:  0.2818\n",
      "  RMSE: 0.3580\n",
      "  R¬≤:   -0.7186\n",
      "\n",
      "üìâ Arousal Regression:\n",
      "  MAE:  0.2386\n",
      "  RMSE: 0.2995\n",
      "  R¬≤:   -0.4494\n",
      "======================================================================\n",
      "\n",
      "\n",
      "  ‚ö† No improvement (9/15)\n",
      "     Best F1: 0.0124 | Best Top-5: 26.51%\n",
      "\n",
      "======================================================================\n",
      "Epoch 11/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:55<00:00,  1.08it/s, Loss=4.2142, E=2.008, V=0.220, A=0.174]\n",
      "Validation Epoch 11: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:04<00:00,  3.06it/s, Loss=8.4906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 11 Summary:\n",
      "  Train - Loss: 4.3649 | Top-1: 15.23% | F1: 0.0595\n",
      "          Top-5: 31.28% | Max Prob: 0.267\n",
      "  Val   - Loss: 8.9798 | Top-1: 0.00% | F1: 0.0000\n",
      "          Top-5: 0.21% | Max Prob: 0.267\n",
      "\n",
      "  ‚ö† No improvement (10/15)\n",
      "     Best F1: 0.0124 | Best Top-5: 26.51%\n",
      "\n",
      "======================================================================\n",
      "Epoch 12/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:55<00:00,  1.08it/s, Loss=3.7334, E=1.772, V=0.139, A=0.239]\n",
      "Validation Epoch 12: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:04<00:00,  3.01it/s, Loss=8.4551]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 12 Summary:\n",
      "  Train - Loss: 4.3660 | Top-1: 14.90% | F1: 0.0586\n",
      "          Top-5: 32.19% | Max Prob: 0.272\n",
      "  Val   - Loss: 8.9107 | Top-1: 0.00% | F1: 0.0000\n",
      "          Top-5: 0.10% | Max Prob: 0.254\n",
      "\n",
      "  ‚ö† No improvement (11/15)\n",
      "     Best F1: 0.0124 | Best Top-5: 26.51%\n",
      "\n",
      "======================================================================\n",
      "Epoch 13/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:55<00:00,  1.08it/s, Loss=3.7092, E=1.758, V=0.198, A=0.186]\n",
      "Validation Epoch 13: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:04<00:00,  3.03it/s, Loss=8.5706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 13 Summary:\n",
      "  Train - Loss: 4.2699 | Top-1: 15.21% | F1: 0.0652\n",
      "          Top-5: 31.95% | Max Prob: 0.278\n",
      "  Val   - Loss: 9.0322 | Top-1: 0.00% | F1: 0.0000\n",
      "          Top-5: 0.10% | Max Prob: 0.262\n",
      "\n",
      "  ‚ö† No improvement (12/15)\n",
      "     Best F1: 0.0124 | Best Top-5: 26.51%\n",
      "\n",
      "======================================================================\n",
      "Epoch 14/100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 45/60 [00:42<00:13,  1.13it/s, Loss=3.6797, E=1.754, V=0.184, A=0.162]"
     ]
    }
   ],
   "source": [
    "   config = {\n",
    "        'train_csv': '/kaggle/input/emotic/normalized_balanced_limited_dominant_emotion.csv',\n",
    "    'val_csv': '/kaggle/input/emotic/normalized_dataset_val_dominant_emotion.csv',\n",
    "    'img_arrays_dir': '/kaggle/input/emotic/archive_emot 2/archive_emot/img_arrs',\n",
    "    'save_path': '/kaggle/working/',\n",
    "       'pretrained_path': '/kaggle/input/emotic/emonet_8.pth',\n",
    "    'freeze_backbone': False,\n",
    "    'image_size': 256,\n",
    "    \n",
    "    \n",
    "        \n",
    "        'num_epochs': 100,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 1e-4,\n",
    "        'weight_decay': 1e-4,\n",
    "        'image_size': 256,\n",
    "        'num_workers': 4,\n",
    "        \n",
    "        # Loss weights\n",
    "        'emotion_loss_weight': 2.0,\n",
    "        'valence_loss_weight': 0.5,\n",
    "        'arousal_loss_weight': 0.5,\n",
    "        \n",
    "        # Class weighting\n",
    "        'weight_method': 'effective_number',  # or 'inverse_freq', 'sqrt_inv_freq'\n",
    "        'weight_beta': 0.9999,\n",
    "        \n",
    "        # ‚úÖ Top-K metrics\n",
    "        'top_k_values': [1, 3, 5],\n",
    "        \n",
    "        'max_samples': None  # for debugging\n",
    "    }\n",
    "\n",
    "model, history = train_emonet_with_probability_analysis(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SimpleInference:\n",
    "    \"\"\"Simple inference for emotion recognition\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataloader):\n",
    "        \"\"\"\n",
    "        Run inference and compute metrics\n",
    "        \n",
    "        Returns:\n",
    "            dict with accuracy, f1, mae_valence, rmse_valence, mae_arousal, rmse_arousal\n",
    "        \"\"\"\n",
    "        all_emotion_preds = []\n",
    "        all_emotion_labels = []\n",
    "        all_valence_preds = []\n",
    "        all_valence_labels = []\n",
    "        all_arousal_preds = []\n",
    "        all_arousal_labels = []\n",
    "        \n",
    "        print(\"Running inference...\")\n",
    "        for batch in tqdm(dataloader):\n",
    "            images = batch['image'].to(self.device)\n",
    "            \n",
    "            # Get one-hot emotion labels (shape: B x 26)\n",
    "            emotion_labels = batch['emotion']  # B x 26\n",
    "            \n",
    "            # Get continuous labels\n",
    "            valence_labels = batch['valence']  # B\n",
    "            arousal_labels = batch['arousal']   # B\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(images)\n",
    "            \n",
    "            # Emotion predictions\n",
    "            emotion_logits = outputs['expression']  # B x 26\n",
    "            emotion_probs = F.softmax(emotion_logits, dim=1)  # B x 26\n",
    "            \n",
    "            # Get argmax (dominant emotion)\n",
    "            emotion_pred_class = torch.argmax(emotion_probs, dim=1)  # B\n",
    "            \n",
    "            # Get ground truth class from one-hot encoding\n",
    "            emotion_true_class = torch.argmax(emotion_labels, dim=1)  # B\n",
    "            \n",
    "            # Store predictions\n",
    "            all_emotion_preds.extend(emotion_pred_class.cpu().numpy())\n",
    "            all_emotion_labels.extend(emotion_true_class.cpu().numpy())\n",
    "            \n",
    "            # Valence and Arousal predictions\n",
    "            all_valence_preds.extend(outputs['valence'].cpu().numpy())\n",
    "            all_valence_labels.extend(valence_labels.numpy())\n",
    "            \n",
    "            all_arousal_preds.extend(outputs['arousal'].cpu().numpy())\n",
    "            all_arousal_labels.extend(arousal_labels.numpy())\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        emotion_preds = np.array(all_emotion_preds)\n",
    "        emotion_labels = np.array(all_emotion_labels)\n",
    "        \n",
    "        valence_preds = np.array(all_valence_preds)\n",
    "        valence_labels = np.array(all_valence_labels)\n",
    "        \n",
    "        arousal_preds = np.array(all_arousal_preds)\n",
    "        arousal_labels = np.array(all_arousal_labels)\n",
    "        \n",
    "        # Compute metrics\n",
    "        print(\"\\nComputing metrics...\")\n",
    "        \n",
    "        # Emotion metrics\n",
    "        accuracy = accuracy_score(emotion_labels, emotion_preds)\n",
    "        f1_macro = f1_score(emotion_labels, emotion_preds, average='macro', zero_division=0)\n",
    "        f1_weighted = f1_score(emotion_labels, emotion_preds, average='weighted', zero_division=0)\n",
    "        \n",
    "        # Valence metrics\n",
    "        valence_mae = mean_absolute_error(valence_labels, valence_preds)\n",
    "        valence_rmse = np.sqrt(mean_squared_error(valence_labels, valence_preds))\n",
    "        \n",
    "        # Arousal metrics\n",
    "        arousal_mae = mean_absolute_error(arousal_labels, arousal_preds)\n",
    "        arousal_rmse = np.sqrt(mean_squared_error(arousal_labels, arousal_preds))\n",
    "        \n",
    "        metrics = {\n",
    "            'emotion_accuracy': accuracy,\n",
    "            'emotion_f1_macro': f1_macro,\n",
    "            'emotion_f1_weighted': f1_weighted,\n",
    "            'valence_mae': valence_mae,\n",
    "            'valence_rmse': valence_rmse,\n",
    "            'arousal_mae': arousal_mae,\n",
    "            'arousal_rmse': arousal_rmse\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{'EVALUATION RESULTS':^60}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nEmotion Classification (26 classes):\")\n",
    "        print(f\"  Accuracy:      {accuracy*100:6.2f}%\")\n",
    "        print(f\"  F1 (Macro):    {f1_macro:6.4f}\")\n",
    "        print(f\"  F1 (Weighted): {f1_weighted:6.4f}\")\n",
    "        print(f\"\\nValence Regression:\")\n",
    "        print(f\"  MAE:           {valence_mae:6.4f}\")\n",
    "        print(f\"  RMSE:          {valence_rmse:6.4f}\")\n",
    "        print(f\"\\nArousal Regression:\")\n",
    "        print(f\"  MAE:           {arousal_mae:6.4f}\")\n",
    "        print(f\"  RMSE:          {arousal_rmse:6.4f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# ============================================\n",
    "# USAGE\n",
    "# ============================================\n",
    "\n",
    "def run_inference(model_path, test_csv, img_arrays_dir, \n",
    "                  batch_size=64, num_workers=4, image_size=256):\n",
    "    \"\"\"\n",
    "    Run inference on test set\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to model checkpoint\n",
    "        test_csv: Path to test CSV\n",
    "        img_arrays_dir: Directory with image arrays\n",
    "        batch_size: Batch size\n",
    "        num_workers: Number of workers\n",
    "        image_size: Image size\n",
    "    \n",
    "    Returns:\n",
    "        dict with all metrics\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    model = EmoNetSingleLabel26(n_expression=26, n_reg=2, attention=True)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(\"‚úì Model loaded\\n\")\n",
    "    \n",
    "    # Create dataset\n",
    "    print(f\"Loading test dataset from {test_csv}...\")\n",
    "    test_dataset = EMOTICDataset(\n",
    "        csv_file=test_csv,\n",
    "        img_arrays_dir=img_arrays_dir,\n",
    "        image_size=image_size,\n",
    "        transform=None\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Test dataset loaded: {len(test_dataset)} samples\\n\")\n",
    "    \n",
    "    # Run inference\n",
    "    inference = SimpleInference(model, device=device)\n",
    "    metrics = inference.evaluate(test_loader)\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    " metrics = run_inference(\n",
    "        model_path='/kaggle/working/best_model.pth',\n",
    "        test_csv='/path/to/test.csv',\n",
    "        img_arrays_dir='/path/to/img_arrays',\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        image_size=256\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úì Inference complete!\")\n",
    "    print(f\"  Emotion Accuracy: {metrics['emotion_accuracy']*100:.2f}%\")\n",
    "    print(f\"  Emotion F1:       {metrics['emotion_f1_macro']:.4f}\")\n",
    "    print(f\"  Valence MAE:      {metrics['valence_mae']:.4f}\")\n",
    "    print(f\"  Valence RMSE:     {metrics['valence_rmse']:.4f}\")\n",
    "    print(f\"  Arousal MAE:      {metrics['arousal_mae']:.4f}\")\n",
    "    print(f\"  Arousal RMSE:     {metrics['arousal_rmse']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8464875,
     "sourceId": 13478449,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
